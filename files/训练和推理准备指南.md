# LISA 训练和推理完整准备指南

## 📋 目录
1. [推理所需准备](#推理所需准备)
2. [训练所需准备](#训练所需准备)
3. [训练参数详解](#训练参数详解)
4. [资源需求](#资源需求)

---

## 🔍 推理所需准备

### 1. 预训练模型

#### 必需模型
- **LISA模型权重**（二选一）：
  - `xinlai/LISA-13B-llama2-v1` （标准版）
  - `xinlai/LISA-13B-llama2-v1-explanatory` （解释版）
  - 或从HuggingFace下载：`xinlai/LISA-7B-v1`、`xinlai/LISA-7B-v1-explanatory`

#### 模型会自动下载
- **CLIP视觉编码器**：`openai/clip-vit-large-patch14`（自动从HuggingFace下载）

### 2. 推理命令

#### 命令行推理（chat.py）
```bash
# 13B模型，16-bit精度（需要30G VRAM）
CUDA_VISIBLE_DEVICES=0 python chat.py --version='xinlai/LISA-13B-llama2-v1'

# 使用bf16精度
CUDA_VISIBLE_DEVICES=0 python chat.py --version='xinlai/LISA-13B-llama2-v1' --precision='bf16'

# 使用8-bit量化（需要16G VRAM）
CUDA_VISIBLE_DEVICES=0 python chat.py --version='xinlai/LISA-13B-llama2-v1' --precision='fp16' --load_in_8bit

# 使用4-bit量化（需要9G VRAM，推荐）
CUDA_VISIBLE_DEVICES=0 python chat.py --version='xinlai/LISA-13B-llama2-v1' --precision='fp16' --load_in_4bit
```

#### Web界面推理（app.py）
```bash
# 默认使用4-bit量化
CUDA_VISIBLE_DEVICES=0 python app.py --version='xinlai/LISA-13B-llama2-v1' --load_in_4bit

# 8-bit推理
CUDA_VISIBLE_DEVICES=0 python app.py --version='xinlai/LISA-13B-llama2-v1' --load_in_8bit

# 16-bit推理（不使用量化）
CUDA_VISIBLE_DEVICES=0 python app.py --version='xinlai/LISA-13B-llama2-v1'
```

### 3. 推理资源需求

| 模型大小 | 精度 | VRAM需求 | 适用场景 |
|---------|------|---------|---------|
| 13B | fp32/bf16/fp16 | ~30G | 单卡A100 40G |
| 13B | 8-bit | ~16G | 单卡RTX 3090/4090 24G |
| 13B | 4-bit | ~9G | 单卡RTX 3060/3070 12G |
| 7B | fp16 | ~15G | 单卡RTX 3090/4090 24G |
| 7B | 4-bit | ~6G | 单卡RTX 3060 12G |

---

## 🎓 训练所需准备

### 1. 预训练模型

#### 必需模型

**A. LLaVA基础模型**（需要先合并delta权重）

根据要训练的模型大小选择：

- **7B模型**：
  - Delta权重：`liuhaotian/LLaVA-Lightning-7B-delta-v1-1`
  - 基础LLM：需要下载对应的LLaMA-7B权重
  - 合并后得到：`LLaVA-Lightning-7B-v1-1`

- **13B模型（Llama2）**：
  - 可直接使用：`liuhaotian/llava-llama-2-13b-chat-lightning-preview`
  - 或使用delta权重：`liuhaotian/LLaVA-13b-delta-v1-1` + 基础LLM

**合并LLaVA权重的方法**：
参考 [LLaVA官方文档](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)

**B. SAM ViT-H权重**
- 下载链接：https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
- 文件大小：约2.4GB
- 保存路径：需要指定 `--vision_pretrained` 参数

### 2. 训练数据集

#### 数据集组织结构

```
dataset/
├── ade20k/                    # 语义分割数据集
│   ├── annotations/
│   └── images/
├── coco/                      # COCO图像（用于语义分割）
│   └── train2017/
│       ├── 000000000009.jpg
│       └── ...
├── cocostuff/                  # COCO-Stuff语义分割
│   └── train2017/
│       ├── 000000000009.png
│       └── ...
├── llava_dataset/              # VQA数据集
│   └── llava_instruct_150k.json
├── mapillary/                  # Mapillary语义分割
│   ├── config_v2.0.json
│   ├── testing/
│   ├── training/
│   └── validation/
├── reason_seg/                 # 推理分割数据集（核心）
│   └── ReasonSeg/
│       ├── train/              # 239张图像
│       ├── val/                # 200张图像
│       └── explanatory/
├── refer_seg/                  # 引用分割数据集
│   ├── images/
│   │   ├── saiapr_tc-12/
│   │   └── mscoco/
│   │       └── images/
│   │           └── train2014/
│   ├── refclef/
│   ├── refcoco/
│   ├── refcoco+/
│   └── refcocog/
└── vlpart/                     # 部件分割数据集
    ├── paco/
    │   └── annotations/
    └── pascal_part/
        ├── train.json
        └── VOCdevkit/
```

#### 数据集下载链接

**1. 语义分割数据集**
- **ADE20K**: http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip
- **COCO-Stuff**: http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/stuffthingmaps_trainval2017.zip
- **COCO Images**: http://images.cocodataset.org/zips/train2017.zip
- **Mapillary**: https://www.mapillary.com/dataset/vistas
- **PACO-LVIS**: https://github.com/facebookresearch/paco/tree/main#dataset-setup
- **PASCAL-Part**: https://github.com/facebookresearch/VLPart/tree/main/datasets#pascal-part

**2. 引用分割数据集**
- **refCOCO**: https://web.archive.org/web/20220413011718/https://bvisionweb1.cs.unc.edu/licheng/referit/data/refcoco.zip
- **refCOCO+**: https://web.archive.org/web/20220413011656/https://bvisionweb1.cs.unc.edu/licheng/referit/data/refcoco+.zip
- **refCOCOg**: https://web.archive.org/web/20220413012904/https://bvisionweb1.cs.unc.edu/licheng/referit/data/refcocog.zip
- **refCLEF**: https://web.archive.org/web/20220413011817/https://bvisionweb1.cs.unc.edu/licheng/referit/data/refclef.zip
- **saiapr_tc-12**: https://web.archive.org/web/20220515000000/http://bvisionweb1.cs.unc.edu/licheng/referit/data/images/saiapr_tc-12.zip

**备用下载**（如果官方链接慢）：
- OneDrive: https://mycuhk-my.sharepoint.com/:f:/g/personal/1155154502_link_cuhk_edu_hk/Em5yELVBvfREodKC94nOFLoBLro_LPxsOxNV44PHRWgLcA?e=zQPjsc

**3. VQA数据集**
- **LLaVA-Instruct-150k**: https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_150k.json

**4. 推理分割数据集（ReasonSeg）**
- 下载链接：https://drive.google.com/drive/folders/125mewyg5Ao6tZ3ZdJ-1-E3n04LGVELqy?usp=sharing
- 包含：239张训练图像 + 200张验证图像 + 779张测试图像
- 格式：每张图像对应一个JSON标注文件

### 3. 训练命令

#### 基础训练命令
```bash
deepspeed --master_port=24999 train_ds.py \
  --version="PATH_TO_LLaVA" \
  --dataset_dir='./dataset' \
  --vision_pretrained="PATH_TO_SAM/sam_vit_h_4b8939.pth" \
  --dataset="sem_seg||refer_seg||vqa||reason_seg" \
  --sample_rates="9,3,3,1" \
  --exp_name="lisa-7b"
```

#### 参数说明
- `--version`: LLaVA模型路径（合并后的完整模型）
- `--vision_pretrained`: SAM ViT-H权重路径
- `--dataset`: 使用的数据集类型，用`||`分隔
- `--sample_rates`: 各数据集的采样率（对应sem_seg, refer_seg, vqa, reason_seg）
- `--exp_name`: 实验名称，用于保存checkpoint

#### 完整训练示例
```bash
# 训练7B模型
deepspeed --master_port=24999 train_ds.py \
  --version="./LLaVA/LLaVA-Lightning-7B-v1-1" \
  --dataset_dir='./dataset' \
  --vision_pretrained="./weights/sam_vit_h_4b8939.pth" \
  --dataset="sem_seg||refer_seg||vqa||reason_seg" \
  --sample_rates="9,3,3,1" \
  --sem_seg_data="ade20k||cocostuff||pascal_part||paco_lvis||mapillary" \
  --refer_seg_data="refclef||refcoco||refcoco+||refcocog" \
  --vqa_data="llava_instruct_150k" \
  --reason_seg_data="ReasonSeg|train" \
  --val_dataset="ReasonSeg|val" \
  --exp_name="lisa-7b" \
  --epochs=10 \
  --batch_size=2 \
  --grad_accumulation_steps=10 \
  --lr=0.0003 \
  --precision="bf16"
```

### 4. 训练后处理

#### 合并LoRA权重
训练完成后，需要将LoRA权重合并到基础模型中：

```bash
CUDA_VISIBLE_DEVICES="" python merge_lora_weights_and_save_hf_model.py \
  --version="./LLaVA/LLaVA-Lightning-7B-v1-1" \
  --weight="./runs/lisa-7b/ckpt_model/pytorch_model.bin" \
  --save_path="./LISA-7B"
```

**注意**：训练完成后，需要先从DeepSpeed checkpoint中提取权重：
```bash
cd ./runs/lisa-7b/ckpt_model && python zero_to_fp32.py . ../pytorch_model.bin
```

### 5. 验证
```bash
deepspeed --master_port=24999 train_ds.py \
  --version="./LISA-7B" \
  --dataset_dir='./dataset' \
  --vision_pretrained="./weights/sam_vit_h_4b8939.pth" \
  --exp_name="lisa-7b" \
  --eval_only
```

---

## 🔧 训练参数详解

### 主要训练参数

#### 1. 可训练模块（Trainable Parameters）

根据代码分析，以下模块会被训练：

**A. LoRA适配器**（默认r=8）
- 目标模块：`q_proj`, `v_proj`（在LLM的attention层）
- LoRA配置：
  - `lora_r=8`：LoRA的秩
  - `lora_alpha=16`：LoRA的缩放因子
  - `lora_dropout=0.05`：Dropout率
  - `lora_target_modules="q_proj,v_proj"`：目标模块

**B. 分割相关模块**
- `mask_decoder`：SAM的mask解码器（如果`--train_mask_decoder=True`）
- `text_hidden_fcs`：文本到分割特征的投影层
  - 结构：`Linear(hidden_size, hidden_size) -> ReLU -> Linear(hidden_size, out_dim)`
  - `out_dim=256`：输出维度

**C. 语言模型头部**
- `lm_head`：语言模型输出层
- `embed_tokens`：词嵌入层

#### 2. 冻结模块（Frozen Parameters）

以下模块在训练时被冻结：
- `vision_tower`：CLIP视觉编码器（完全冻结）
- `mm_projector`：多模态投影层（冻结）
- `visual_model.image_encoder`：SAM的图像编码器（冻结）

#### 3. 训练超参数

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `epochs` | 10 | 训练轮数 |
| `steps_per_epoch` | 500 | 每轮步数 |
| `batch_size` | 2 | 每GPU批大小 |
| `grad_accumulation_steps` | 10 | 梯度累积步数（有效batch=2×10×GPU数） |
| `lr` | 0.0003 | 学习率 |
| `precision` | bf16 | 精度（bf16/fp16/fp32） |
| `lora_r` | 8 | LoRA秩 |
| `lora_alpha` | 16 | LoRA缩放因子 |
| `lora_dropout` | 0.05 | LoRA dropout |
| `ce_loss_weight` | 1.0 | 交叉熵损失权重 |
| `dice_loss_weight` | 0.5 | Dice损失权重 |
| `bce_loss_weight` | 2.0 | 二元交叉熵损失权重 |

#### 4. 数据集采样率

默认采样率 `--sample_rates="9,3,3,1"` 表示：
- `sem_seg`（语义分割）：9
- `refer_seg`（引用分割）：3
- `vqa`（视觉问答）：3
- `reason_seg`（推理分割）：1

这意味着在混合数据集中，语义分割数据会被更频繁地采样。

---

## 💻 资源需求

### 训练资源需求

#### 硬件要求

**最小配置**（单卡训练）：
- GPU: 1× A100 40GB 或 2× RTX 3090 24GB
- RAM: 64GB+
- 存储: 500GB+（用于数据集和checkpoint）

**推荐配置**（多卡训练）：
- GPU: 4× A100 40GB 或 8× RTX 3090 24GB
- RAM: 128GB+
- 存储: 1TB+ SSD

#### 训练时间估算

基于默认配置（10 epochs, 500 steps/epoch, batch_size=2, grad_accum=10）：

| GPU配置 | 每步时间 | 总训练时间 |
|--------|---------|-----------|
| 1× A100 | ~2-3秒 | ~3-4小时 |
| 4× A100 | ~0.5-1秒 | ~1-2小时 |
| 8× RTX 3090 | ~1-2秒 | ~2-3小时 |

**注意**：实际时间取决于数据集大小和加载速度。

#### 存储空间需求

- **数据集**：约200-300GB
  - ADE20K: ~2GB
  - COCO: ~20GB
  - Mapillary: ~50GB
  - refCOCO系列: ~10GB
  - ReasonSeg: ~500MB
  - 其他: ~100GB

- **模型权重**：
  - LLaVA-7B: ~14GB
  - LLaVA-13B: ~26GB
  - SAM ViT-H: ~2.4GB
  - 训练checkpoint: ~50-100GB（DeepSpeed ZeRO-2）

- **总计**：约300-400GB

### 推理资源需求

见[推理资源需求](#3-推理资源需求)部分。

---

## 📝 注意事项

### 训练注意事项

1. **LLaVA权重合并**：
   - 必须先从delta权重合并得到完整LLaVA模型
   - 不能直接使用delta权重进行训练

2. **数据集准备**：
   - 确保所有数据集按照指定目录结构组织
   - ReasonSeg数据集必须包含train和val目录
   - JSON标注文件格式必须正确

3. **DeepSpeed配置**：
   - 使用ZeRO Stage 2优化
   - 支持多GPU分布式训练
   - 自动保存checkpoint

4. **精度选择**：
   - 推荐使用`bf16`（A100/H100）
   - 较老GPU使用`fp16`
   - `fp32`训练速度慢但最稳定

### 推理注意事项

1. **量化影响**：
   - 4-bit量化会轻微影响文本生成质量
   - 8-bit量化影响较小
   - 16-bit精度最佳但显存需求大

2. **模型版本**：
   - `v1`模型在train+val上训练，用于最终评估
   - `v0`模型仅在train上训练，用于验证集评估
   - 使用`v0`模型需要checkout到legacy版本：`git checkout 0e26916`

3. **提示词格式**：
   - 标准格式："Can you segment xxx in this image?"
   - 带解释："What is xxx? Please output segmentation mask and explain why."
   - 注意标点符号和语法正确性

---

## 🔗 相关链接

- **论文**: https://arxiv.org/abs/2308.00692
- **HuggingFace模型**: https://huggingface.co/xinlai
- **LLaVA官方**: https://github.com/haotian-liu/LLaVA
- **SAM官方**: https://github.com/facebookresearch/segment-anything
- **ReasonSeg数据集**: https://drive.google.com/drive/folders/125mewyg5Ao6tZ3ZdJ-1-E3n04LGVELqy?usp=sharing

---

## 📊 总结

### 快速开始检查清单

**推理**：
- [ ] 安装依赖：`pip install -r requirements.txt`
- [ ] 下载LISA模型（自动或手动）
- [ ] 运行 `chat.py` 或 `app.py`

**训练**：
- [ ] 安装依赖：`pip install -r requirements.txt && pip install flash-attn --no-build-isolation`
- [ ] 下载并合并LLaVA权重
- [ ] 下载SAM ViT-H权重
- [ ] 下载并组织所有训练数据集
- [ ] 配置DeepSpeed环境
- [ ] 运行训练脚本
- [ ] 合并LoRA权重
- [ ] 验证模型

---

*最后更新：基于LISA仓库最新代码分析*

